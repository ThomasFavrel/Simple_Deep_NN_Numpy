{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ae4d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002da3b0",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff14290",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "# Flatten\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "390053a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0. It's a non-cat picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 8\n",
    "plt.imshow(train_x_orig[index])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4d4ec",
   "metadata": {},
   "source": [
    "## 1. Initialize parameters\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> <b>Shape of W</b> </td> \n",
    "        <td> <b>Shape of b</b>  </td> \n",
    "        <td> <b>Activation</b> </td>\n",
    "        <td> <b>Shape of Activation</b> </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> <b>Layer 1</b> </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> <b>Layer 2</b> </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>  \n",
    "   <tr>\n",
    "       <td> <b>Layer L-1</b> </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "   <tr>\n",
    "   <tr>\n",
    "       <td> <b>Layer L</b> </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8426a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046a5cc",
   "metadata": {},
   "source": [
    "## 2. Forward Porpagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15fa8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d194ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e95a6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78c5bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352d61bf",
   "metadata": {},
   "source": [
    "## 3. Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f79a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed75661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf3b5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c634e41",
   "metadata": {},
   "source": [
    "## 4. Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f05f1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446fecd",
   "metadata": {},
   "source": [
    "## 5. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd7822de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1575820",
   "metadata": {},
   "source": [
    "## 6. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6ef738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ab70861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.7717493284237686\n",
      "Cost after iteration 100: 0.6720534400822914\n",
      "Cost after iteration 200: 0.6482632048575212\n",
      "Cost after iteration 300: 0.6115068816101354\n",
      "Cost after iteration 400: 0.5670473268366111\n",
      "Cost after iteration 500: 0.5401376634547801\n",
      "Cost after iteration 600: 0.5279299569455267\n",
      "Cost after iteration 700: 0.4654773771766851\n",
      "Cost after iteration 800: 0.36912585249592794\n",
      "Cost after iteration 900: 0.39174697434805344\n",
      "Cost after iteration 1000: 0.3151869888600617\n",
      "Cost after iteration 1100: 0.2726998441789385\n",
      "Cost after iteration 1200: 0.23741853400268137\n",
      "Cost after iteration 1300: 0.19960120532208644\n",
      "Cost after iteration 1400: 0.18926300388463305\n",
      "Cost after iteration 1500: 0.1611885466582775\n",
      "Cost after iteration 1600: 0.14821389662363318\n",
      "Cost after iteration 1700: 0.13777487812972944\n",
      "Cost after iteration 1800: 0.12974017549190123\n",
      "Cost after iteration 1900: 0.12122535068005211\n",
      "Cost after iteration 2000: 0.11382060668633712\n",
      "Cost after iteration 2100: 0.10783928526254133\n",
      "Cost after iteration 2200: 0.10285466069352679\n",
      "Cost after iteration 2300: 0.10089745445261787\n",
      "Cost after iteration 2400: 0.09287821526472397\n",
      "Cost after iteration 2499: 0.08843994344170202\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "parameters, costs = L_layer_model(train_x,\n",
    "                                  train_y,\n",
    "                                  layers_dims,\n",
    "                                  learning_rate=0.0075,\n",
    "                                  num_iterations=2500,\n",
    "                                  print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe51443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9856459330143539\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b198ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e526f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, 'relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, 'sigmoid')\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'relu')\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs\n",
    "\n",
    "def plot_costs(costs, learning_rate=0.0075):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61726d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 12288     # num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)\n",
    "learning_rate = 0.0075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bc1952a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693049735659989\n",
      "Cost after iteration 100: 0.6464320953428849\n",
      "Cost after iteration 200: 0.6325140647912678\n",
      "Cost after iteration 300: 0.6015024920354665\n",
      "Cost after iteration 400: 0.5601966311605747\n",
      "Cost after iteration 500: 0.5158304772764729\n",
      "Cost after iteration 600: 0.4754901313943325\n",
      "Cost after iteration 700: 0.43391631512257495\n",
      "Cost after iteration 800: 0.400797753620389\n",
      "Cost after iteration 900: 0.3580705011323798\n",
      "Cost after iteration 1000: 0.33942815383664116\n",
      "Cost after iteration 1100: 0.3052753636196263\n",
      "Cost after iteration 1200: 0.2749137728213018\n",
      "Cost after iteration 1300: 0.24681768210614857\n",
      "Cost after iteration 1400: 0.19850735037466102\n",
      "Cost after iteration 1500: 0.1744831811255665\n",
      "Cost after iteration 1600: 0.17080762978096578\n",
      "Cost after iteration 1700: 0.11306524562164716\n",
      "Cost after iteration 1800: 0.09629426845937145\n",
      "Cost after iteration 1900: 0.08342617959726861\n",
      "Cost after iteration 2000: 0.0743907870431908\n",
      "Cost after iteration 2100: 0.06630748132267929\n",
      "Cost after iteration 2200: 0.05919329501038164\n",
      "Cost after iteration 2300: 0.05336140348560554\n",
      "Cost after iteration 2400: 0.04855478562877016\n",
      "Cost after iteration 2499: 0.04421498215868952\n"
     ]
    }
   ],
   "source": [
    "parameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c22a596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d62135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "predictions_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d17ddcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
